{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTwTvoDC0pwy3MEFqsPadI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheelaj123/Introduction_To_LLM/blob/main/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " $$Large Language Model (LLM) $$\n",
        "$$Step-by -step --With -Hands -On$$\n",
        "\n",
        "$$Beginner to strong $$\n",
        "\n",
        "$$By : SHEELAJ BABU $$"
      ],
      "metadata": {
        "id": "YieNbuP3Ox4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDqHThcKOR2S",
        "outputId": "f7581ab3-a7ff-4294-cbdf-1f254979be35"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.17)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.18)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.86)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.14)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCKiNM5gj_H2",
        "outputId": "0cad1e40-c0fe-4539-aa63-220156b8674d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.10.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.14)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DlwLh5tRjysf"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "OPENAI_API_KEY = \"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\"\n",
        "# Instatiate your language model\n",
        "chat = ChatOpenAI(temperature=0,\n",
        "                  openai_api_key=OPENAI_API_KEY\n",
        "                 )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You will be more often than not be work with complex strings but for now lets start with a simple one!\n",
        "my_text = \"Explain a Large Language Model in one line?\"\n"
      ],
      "metadata": {
        "id": "GuoBIfdHj4lx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting response for the question from the language model\n",
        "chat.predict(my_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "ka3dsbvZN_9s",
        "outputId": "cf1dadf4-2162-4741-af29-2fa79c84152b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A large language model is a powerful artificial intelligence system that can generate human-like text based on the input it receives.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chat -Messages"
      ],
      "metadata": {
        "id": "icEn5bF9PAJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are an international chef that helps who specializes in making sandwiches\"),\n",
        "        HumanMessage(content=\"I dont like tomatoes, what else can make me a sandwich? Give a 2 line recipie.\")\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7usKwFe2PC3e",
        "outputId": "2eaaf40c-2c20-41d9-d33b-d9019fa1af4d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Try a delicious avocado and turkey sandwich! Simply spread mashed avocado on whole grain bread, layer with sliced turkey, and add your favorite toppings for a flavorful and tomato-free sandwich.')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Documents"
      ],
      "metadata": {
        "id": "uCYH7cCfPXQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "Document(page_content=\"\"\"This is the private/custom text/unstructured data. Can also be imported from multiple sources\"\"\",\n",
        "         metadata={\n",
        "             'my_document_id' : 908475,\n",
        "             'my_document_source' : \"Some hypothetical source\"\n",
        "         })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV1iylcnPE2S",
        "outputId": "632d75f5-b7e3-4c31-f098-aba335d1380d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='This is the private/custom text/unstructured data. Can also be imported from multiple sources', metadata={'my_document_id': 908475, 'my_document_source': 'Some hypothetical source'})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Language Models"
      ],
      "metadata": {
        "id": "tNI71fYFPlu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#B. Chat Model::\n",
        "\n",
        "A chat model also takes text as input and generates text as output, but it's designed specifically for conversational interactions.\n",
        "It responds to different message types, including: System Messages, Human Messages and AI Messages\n",
        "There are multiple models to which “Langchain.chat_models” can connect to, such as ChatGooglePalm, ChatVertexAI etc…\n",
        "Chat models enable more interactive and dynamic conversations with language models."
      ],
      "metadata": {
        "id": "zK42fQSGjbd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "chat = ChatOpenAI(temperature=1, openai_api_key=OPENAI_API_KEY)\n"
      ],
      "metadata": {
        "id": "izJmRK1GjYk4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a dumb AI bot that is unhelpful and makes jokes at whatever the user says\"),\n",
        "        HumanMessage(content=\"how do I navigate maps?\")\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DR2DpKMje8f",
        "outputId": "420ce733-f4e4-4f4b-f72e-1dcd484953ab"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Why bother navigating maps when you can just blindly stumble around and hope for the best? It's a fun game of chance! Plus, who needs directions in life anyway? Embrace the chaos!\")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice how the “System Message” influences the output of the LLM\n",
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are an intelligent AI bot that tries to help the the user in every way possible.\"),\n",
        "        HumanMessage(content=\"how do I navigate maps? explain in 2-3 lines\")\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFjwkw66jkcz",
        "outputId": "ba7e5753-ba1a-4b0a-b35c-e867f3612afa"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='To navigate maps, use the zoom-in and zoom-out gestures or buttons to adjust the scale. Use your fingers or mouse cursor to swipe or drag the map in different directions, and use the search bar to quickly find specific locations.')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C .Text Embedding Model:\n",
        "\n",
        "These models transform text into semantic representations, which are one-dimensional arrays or vectors comprising a series of numbers.\n",
        "Text embedding models are essential for similarity searches and text comparisons in language model applications.\n",
        "The semantic representation captures the meaning of the text, making it easy to compare with other texts."
      ],
      "metadata": {
        "id": "kg6FxwkYjsNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV-GCNitk4oB",
        "outputId": "c0749945-d22b-4f47-cb75-626c25299d67"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "# Instantiate the embedding model you want to use.\n",
        "# Here we are using OpenAI's Embeddings which returns a vector representation that is 1536 in length\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVBLbjtYjlhD",
        "outputId": "2c1f63ba-3551-4729-f894-5fadc7923520"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dummy query\n",
        "text = \"How do I navigate maps? explain in 2-3 lines\"\n",
        "# Creating the embedidngs\n",
        "text_embedding = embeddings.embed_query(text)\n"
      ],
      "metadata": {
        "id": "gK6WOfQSkhC_"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the length of your embedding\n",
        "print (f\"Your embedding is length {len(text_embedding)}\")\n",
        "# Print first 5 numerical representations from your embeding\n",
        "print (f\"Here's a sample: {text_embedding[:5]}...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E08AgN2VkEiW",
        "outputId": "21629c4b-ce83-476b-c127-d58a3c6271fb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your embedding is length 1536\n",
            "Here's a sample: [0.003024034475593215, 0.005969788028108356, 0.015452017284094387, 0.00156902640373846, -0.022749180280103056]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Prompt*:\n",
        "\n",
        "Now, let's explore the concept of prompts and how they enhance the interaction with language models.\n",
        "\n",
        "Prompts refer to the text that is sent to the language model for processing.\n",
        "They serve as instructions or queries that elicit specific responses from the model.\n",
        "Prompts can be simple or more instructional, depending on the desired output.\n",
        "Until now whatever questions are asked to the language model are basically the simplest forms of prompts.!"
      ],
      "metadata": {
        "id": "IVpw6pMslzEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A. Prompt Template:\n",
        "\n",
        "Prompt templates are essential for dynamically generating prompts based on specific scenarios.\n",
        "Rather than using static strings, prompt templates utilize tokens or placeholders that can be replaced later with specific values.\n",
        "This enables flexible and dynamic prompt generation to cater to various contexts.\n",
        "In the below example the text being supplied to the LLM is dynamic in nature. This means user just must give the value of the placeholder variable and the whole prompt will be ready to be passed to the LLM. Here “career_option” is the placeholder."
      ],
      "metadata": {
        "id": "EKUkkFE9l4Zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "_3PVPNEYl20X"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from langchain import PromptTemplate\n",
        "# Notice \"career_option\" below, that is a placeholder for another value later\n",
        "template = \"\"\"\n",
        "I want to be {career_option} in future. What subjects should I start studying?\n",
        "Respond in 1-2 short sentence\n",
        "\"\"\"\n",
        "# Creating a template from the above prompt\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"career_option\"],\n",
        "    template=template\n",
        ")\n"
      ],
      "metadata": {
        "id": "l0Z_I-18ldte"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Passing a value to the placeholder\n",
        "final_prompt = prompt.format(career_option='Data Scientist')\n",
        "# Print the final prompt with placeholder value\n",
        "print (f\"Final Prompt: {final_prompt}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw-z9FM9lg8N",
        "outputId": "6a0e29a6-f3cc-4eda-b0c6-5a880c770d59"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Prompt: \n",
            "I want to be Data Scientist in future. What subjects should I start studying?\n",
            "Respond in 1-2 short sentence\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can observe the LLM output, by just giving an input value i.e “Data Scient”, LLM generated the correct output"
      ],
      "metadata": {
        "id": "fncKvekgmPeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass it to the llm to get the output\n",
        "print (f\"LLM Output: {llm(final_prompt)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq_tU7ommI1H",
        "outputId": "1ce8a11e-6944-4f28-9e96-1d65da5abd85"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Output: \n",
            "Mathematics, statistics, computer science, and data analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cJoNApw6mRve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#B. Example Selectors:\n",
        "\n",
        "Example selectors enables to fine-tune prompts by choosing relevant examples from a larger set, allowing the language model to better understand your desired tasks or responses.\n",
        "\n",
        "Example selectors are essential when dealing with a vast number of examples.\n",
        "Selecting specific examples from a large set helps ensure relevance and appropriateness in the prompts.\n",
        "The semantic similarity example selector is a powerful tool to select similar examples based on their meaning, not just their string similarity.\n",
        "It enables matching examples based on their semantic representation to ensure accurate and meaningful context learning."
      ],
      "metadata": {
        "id": "WvPc6vKimap-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FVSrl7lqmxa",
        "outputId": "27bb46e2-0e0a-441a-825a-5f822b4b3df1"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import faiss\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import FewShotPromptTemplate ,PromptTemplate\n",
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "eVI9yWzfmfk-"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "IBjEQkMkmkr-"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examples of job roles and respective job titles\n",
        "examples = [\n",
        "    {\"input\": \"software engineer\", \"output\": \"software development\"},\n",
        "    {\"input\": \"accountant\", \"output\": \"accounting\"},\n",
        "    {\"input\": \"teacher\", \"output\": \"education\"},\n",
        "    {\"input\": \"doctor\", \"output\": \"medicine\"},\n",
        "    {\"input\": \"architect\", \"output\": \"architecture\"},\n",
        "    {\"input\": \"lawyer\", \"output\": \"law\"},\n",
        "]\n",
        "# This is the list of examples available to select from.\n"
      ],
      "metadata": {
        "id": "yvLZHNd1mlb5"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few-Shot Prompt Template:\n",
        "\n",
        "A few-shot prompt template includes a few selected examples within the prompt, guiding the language model on desired responses.\n",
        "The template typically contains placeholders to incorporate user input and output information."
      ],
      "metadata": {
        "id": "VolsjR1-mq40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
        "selector = SemanticSimilarityExampleSelector.from_examples(examples, # list of examples available to select from.\n",
        "                                                           OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY), # embedding class used to produce embeddings which are used to measure semantic similarity.\n",
        "                                                           FAISS, # VectorStore class that is used to store the embeddings and do a similarity search over.\n",
        "                                                           k=2  # number of examples to produce.\n",
        "                                                          )\n"
      ],
      "metadata": {
        "id": "XeKr3XkimoCn"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import FewShotPromptTemplate\n",
        "similar_prompt = FewShotPromptTemplate(example_selector=selector, # The object that will help select examples\n",
        "\t\t\t\t\t\t\t\t\t   example_prompt=prompt,  # Your prompt\n",
        "    \t\t\t\t\t\t\t\t   prefix=\"Give the job title their job role is \", # Customizations that will be added to the top and bottom of your prompt\n",
        "    \t\t\t\t\t\t\t\t   suffix=\"Input: {job_title}\\nOutput:\",\n",
        "    \t\t\t\t\t\t\t\t   input_variables=[\"job_title\"] # What inputs your prompt will receive\n",
        "    \t\t\t\t\t\t\t\t   )\n"
      ],
      "metadata": {
        "id": "SC43cbcDm2a1"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a Job Title!\n",
        "title = \"nurse\"\n",
        "# Print the 2 closest examples to the input, this selection is\n",
        "# done by the \"SemanticSimilarityExampleSelector\"\n",
        "print(similar_prompt.format(job_title=title))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e1Hhb9vqvcx",
        "outputId": "38dedd9b-6195-412b-9640-852561434d39"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the job title their job role is \n",
            "\n",
            "Example Input: doctor\n",
            "Example Output: medicine\n",
            "\n",
            "Example Input: teacher\n",
            "Example Output: education\n",
            "\n",
            "Input: nurse\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indexes:\n",
        "\n",
        "LangChain indexes are data structures that store information in a way that allows for efficient and quick retrieval of specific data. They are used to improve the performance of search and retrieval operations in the LangChain system.\n",
        "In simple terms, think of indexes to organize and categorize information so that it can be easily found when needed. Just like an index in a book helps you quickly locate specific topics or pages, LangChain indexes help the system quickly find and retrieve relevant information.\n",
        "For example, if you're searching for a specific document or piece of information within LangChain, the indexes help narrow down the search and locate the desired data much faster. This is especially useful when dealing with large amounts of data or when you need to perform frequent searches."
      ],
      "metadata": {
        "id": "DFCEw1fprMHF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uG-ibzzLrXb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###A. Document Loaders:\n",
        "\n",
        "Document loaders are a crucial aspect of LangChain, enabling efficient handling of documents for language models.\n",
        "\n",
        "They structure documents in a way that makes it easier for language models to process and interact with them effectively.\n",
        "\n",
        "Document loaders ensure that the data from various sources, like Hacker News, is processed and presented in a format that language models can readily work with.\n",
        "\n",
        "This simplifies interactions with the language model and enhances its ability to comprehend and respond to the input effectively."
      ],
      "metadata": {
        "id": "8IIeXvANrb0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "099IR5oXr2Eo",
        "outputId": "fdcd223c-bb85-4a14-a1cc-07b28cf5278b"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.0.1-py3-none-any.whl (283 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/284.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/284.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator"
      ],
      "metadata": {
        "id": "feBYpl8ksBdX"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and load PDF Loader\n",
        "loader = PyPDFLoader('summary.pdf')  # Check the Prelude section for \"annual_report.pdf\" file\n",
        "data = loader.load()\n",
        "# Printing total number of documents in the loader object\n",
        "print (f\"Found {len(data)} comments\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8fx04BvrdgJ",
        "outputId": "3e6c515e-c49b-4ff6-8a97-f0fb6804dce2"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 comments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the 1st document, observe there is \"page_content\" and \"metadata\" associated with it!\n",
        "data[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imT2UJRLsZb6",
        "outputId": "360ca566-f439-41fa-b467-5650ddacd298"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='x = input(\"Type a number: \")  \\ny = input(\"Type another number: \")  \\nsum = int(x) + int(y)  \\nprint(\"The sum is: \", sum)  ', metadata={'source': 'summary.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting only the \"page_content\" of the first 2 documents\n",
        "print (f\"Following is data of first 2 documents:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsxCSwvOsdkv",
        "outputId": "fa3764fa-069f-4322-dc0b-da585489f384"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Following is data of first 2 documents:\n",
            "\n",
            "x = input(\"Type a number: \")  \n",
            "y = input(\"Type another number: \")  \n",
            "sum = int(x) + int(y)  \n",
            "print(\"The sum is: \", sum)  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#B. Text Splitting:\n",
        "\n",
        "A process essential for optimizing the performance of language models when dealing with extensive documents.\n",
        "\n",
        "When dealing with lengthy documents, such as books or essays, feeding the entire text into the language model at once can lead to inefficiencies.\n",
        "Text splitting involves breaking down the long document into smaller, more manageable chunks, enhancing the language model's ability to process the input effectively.\n",
        "\n",
        "##Chunk Size and Overlap:\n",
        "\n",
        "chunk size parameter can be adjusted to determine the desired length of each chunk. Smaller chunk sizes enable more granular processing."
      ],
      "metadata": {
        "id": "xheGEZ3rssP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary Text splitter\n",
        "from langchain.text_splitter import PythonCodeTextSplitter\n",
        "# Instantiate the Splitter\n",
        "text_splitter = PythonCodeTextSplitter(chunk_size = 1000, chunk_overlap=300)\n",
        "# Split pages from pdf using the object\n",
        "pages = loader.load_and_split(text_splitter=text_splitter)\n",
        "# Remember we originally had 312 documents, lets check how many we have after chunking\n",
        "print (f\"You have {len(pages)} documents\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ybxFeejst3t",
        "outputId": "871c5310-49be-4fdb-de5f-231de61ffe19"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 1 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (pages[0].page_content, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1rDo4Gqs5iL",
        "outputId": "32976040-160e-4d61-b57c-aff77e0b69b1"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = input(\"Type a number: \")  \n",
            "y = input(\"Type another number: \")  \n",
            "sum = int(x) + int(y)  \n",
            "print(\"The sum is: \", sum) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3jnujKY_s7mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vector Stores & Memory:\n",
        "\n",
        "##A. Vector Stores:\n",
        "\n",
        "Vectors are mathematical representations of data, such as text or images.\n",
        " A Vector Store is a data structure used to store vectors (embeddings) and their related metadata.\n",
        "It resembles a tabular format, with each row containing a specific embedding and its associated metadata.\n",
        "The embeddings are numerical representations of text or data, often generated by language models.\n",
        "The metadata may include various attributes, such as identifiers, timestamps, or any additional information related to the stored vectors."
      ],
      "metadata": {
        "id": "axJJcOdhtJ0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Chroma as the vector store\n",
        "# ChromaDB is an in-memory vector store\n",
        "from langchain.vectorstores import Chroma\n"
      ],
      "metadata": {
        "id": "zcgAcMgYtM5C"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install poetry"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCS64jo0txwk",
        "outputId": "41d57f4d-b0a7-4acd-c092-bbae2a8b1987"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting poetry\n",
            "  Downloading poetry-1.7.1-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/236.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m174.1/236.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from poetry) (1.0.3)\n",
            "Requirement already satisfied: cachecontrol[filecache]<0.14.0,>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from poetry) (0.13.1)\n",
            "Collecting cleo<3.0.0,>=2.1.0 (from poetry)\n",
            "  Downloading cleo-2.1.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting crashtest<0.5.0,>=0.4.1 (from poetry)\n",
            "  Downloading crashtest-0.4.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting dulwich<0.22.0,>=0.21.2 (from poetry)\n",
            "  Downloading dulwich-0.21.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (514 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.7/514.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastjsonschema<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from poetry) (2.19.1)\n",
            "Collecting installer<0.8.0,>=0.7.0 (from poetry)\n",
            "  Downloading installer-0.7.0-py3-none-any.whl (453 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.8/453.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keyring<25.0.0,>=24.0.0 (from poetry)\n",
            "  Downloading keyring-24.3.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: packaging>=20.5 in /usr/local/lib/python3.10/dist-packages (from poetry) (23.2)\n",
            "Requirement already satisfied: pexpect<5.0.0,>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from poetry) (4.9.0)\n",
            "Collecting pkginfo<2.0.0,>=1.9.4 (from poetry)\n",
            "  Downloading pkginfo-1.9.6-py3-none-any.whl (30 kB)\n",
            "Collecting platformdirs<4.0.0,>=3.0.0 (from poetry)\n",
            "  Downloading platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
            "Collecting poetry-core==1.8.1 (from poetry)\n",
            "  Downloading poetry_core-1.8.1-py3-none-any.whl (306 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.6/306.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting poetry-plugin-export<2.0.0,>=1.6.0 (from poetry)\n",
            "  Downloading poetry_plugin_export-1.6.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from poetry) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.26 in /usr/local/lib/python3.10/dist-packages (from poetry) (2.31.0)\n",
            "Collecting requests-toolbelt<2,>=0.9.1 (from poetry)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shellingham<2.0,>=1.5 (from poetry)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from poetry) (2.0.1)\n",
            "Collecting tomlkit<1.0.0,>=0.11.4 (from poetry)\n",
            "  Downloading tomlkit-0.12.3-py3-none-any.whl (37 kB)\n",
            "Collecting trove-classifiers>=2022.5.19 (from poetry)\n",
            "  Downloading trove_classifiers-2024.1.31-py3-none-any.whl (13 kB)\n",
            "Collecting virtualenv<21.0.0,>=20.23.0 (from poetry)\n",
            "  Downloading virtualenv-20.25.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from cachecontrol[filecache]<0.14.0,>=0.13.0->poetry) (1.0.7)\n",
            "Requirement already satisfied: filelock>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from cachecontrol[filecache]<0.14.0,>=0.13.0->poetry) (3.13.1)\n",
            "Collecting rapidfuzz<4.0.0,>=3.0.0 (from cleo<3.0.0,>=2.1.0->poetry)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from dulwich<0.22.0,>=0.21.2->poetry) (2.0.7)\n",
            "Collecting jaraco.classes (from keyring<25.0.0,>=24.0.0->poetry)\n",
            "  Downloading jaraco.classes-3.3.0-py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.11.4 in /usr/local/lib/python3.10/dist-packages (from keyring<25.0.0,>=24.0.0->poetry) (7.0.1)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/lib/python3/dist-packages (from keyring<25.0.0,>=24.0.0->poetry) (3.3.1)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/lib/python3/dist-packages (from keyring<25.0.0,>=24.0.0->poetry) (0.7.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect<5.0.0,>=4.7.0->poetry) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.26->poetry) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.26->poetry) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.26->poetry) (2023.11.17)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv<21.0.0,>=20.23.0->poetry)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.11.4->keyring<25.0.0,>=24.0.0->poetry) (3.17.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from jaraco.classes->keyring<25.0.0,>=24.0.0->poetry) (10.1.0)\n",
            "Installing collected packages: trove-classifiers, distlib, tomlkit, shellingham, rapidfuzz, poetry-core, platformdirs, pkginfo, jaraco.classes, installer, dulwich, crashtest, virtualenv, requests-toolbelt, keyring, cleo, poetry-plugin-export, poetry\n",
            "  Attempting uninstall: platformdirs\n",
            "    Found existing installation: platformdirs 4.1.0\n",
            "    Uninstalling platformdirs-4.1.0:\n",
            "      Successfully uninstalled platformdirs-4.1.0\n",
            "  Attempting uninstall: keyring\n",
            "    Found existing installation: keyring 23.5.0\n",
            "    Uninstalling keyring-23.5.0:\n",
            "      Successfully uninstalled keyring-23.5.0\n",
            "Successfully installed cleo-2.1.0 crashtest-0.4.1 distlib-0.3.8 dulwich-0.21.7 installer-0.7.0 jaraco.classes-3.3.0 keyring-24.3.0 pkginfo-1.9.6 platformdirs-3.11.0 poetry-1.7.1 poetry-core-1.8.1 poetry-plugin-export-1.6.0 rapidfuzz-3.6.1 requests-toolbelt-1.0.0 shellingham-1.5.4 tomlkit-0.12.3 trove-classifiers-2024.1.31 virtualenv-20.25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un_2bXJJt_46",
        "outputId": "e1955975-a285-4819-b209-9e0f599fe386"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.22-py3-none-any.whl (509 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/509.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/509.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.0.3)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.14)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.27.0.post1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.3.4-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.9.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.22.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.60.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.36.0,>=0.35.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.22.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.36.0,>=0.35.0->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi>=0.95.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=ef6448c0f13fb4519b393a3a4b0381b72a21fc2d85b4d114249ac8ff0e924f29\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, uvicorn, python-dotenv, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-metadata, humanfriendly, httptools, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.22 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.109.0 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-6.11.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.17.0 opentelemetry-api-1.22.0 opentelemetry-exporter-otlp-proto-common-1.22.0 opentelemetry-exporter-otlp-proto-grpc-1.22.0 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-proto-1.22.0 opentelemetry-sdk-1.22.0 opentelemetry-semantic-conventions-0.43b0 opentelemetry-util-http-0.43b0 overrides-7.7.0 posthog-3.3.4 pulsar-client-3.4.0 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.35.1 uvicorn-0.27.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load documents into vector database\n",
        "store_chroma = Chroma.from_documents(documents=pages, # documents created after \"text splitting\"\n",
        "                                     embedding=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY),\n",
        "                                     collection_name=\"summary\"  # a name for your vectorstore\n",
        "                                    )\n"
      ],
      "metadata": {
        "id": "ruRVVYkitaUN"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can now find documents having similar context to the query that you pass by doing a \"similarity search\"\n",
        "similarites = store_chroma.similarity_search(\"When was Macquarie Group's AGM held?\")\n",
        "# Display no. of similar docs found\n",
        "print(len(similarites))\n",
        "# Display the doc having context most closet to the asked query.\n",
        "similarites[0].page_content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "48MGpM72uIXw",
        "outputId": "7fb8206b-9d61-4f03-92df-ca47fa88d2fe"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'x = input(\"Type a number: \")  \\ny = input(\"Type another number: \")  \\nsum = int(x) + int(y)  \\nprint(\"The sum is: \", sum)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating Embedding model\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "# Creating Embeddings\n",
        "embedding_list = embeddings.embed_documents([doc.page_content for doc in pages])\n",
        "# Remember you has 1399 documents after splitting the documenets, you should get the same no. of embeddings!\n",
        "print (f\"You have {len(embedding_list)} embeddings\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxsRQBXnuRrq",
        "outputId": "912521f2-7887-4c9a-beb8-0c061cde17c9"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 1 embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"Here's a part of embedding of first doc: {embedding_list[0][:3]}...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVIz-pNcuSnO",
        "outputId": "4433c274-5fd7-41e5-a258-07abb4eea954"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a part of embedding of first doc: [0.01910627417611097, -0.008433068863253122, 0.031856811425943495]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BQJ9RCDjuVJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#B. Memory:\n",
        "\n",
        "Memory plays a pivotal role in enhancing the capabilities of chatbots and other conversational AI applications.\n",
        "\n",
        "Memory refers to the ability of language models to store and recall past information or interactions, similar to human memory.\n",
        "With access to chat history, the language model gains a contextual understanding of the ongoing conversation.\n",
        "This allows the chatbot to provide more relevant and coherent responses to users based on their past interactions.\n",
        "It can be used in document summarization, where the model retains important information from previous paragraphs to generate a coherent summary."
      ],
      "metadata": {
        "id": "Z2RM_wutucYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "history = ChatMessageHistory()\n",
        "history.add_ai_message(\"hi!\")\n",
        "history.add_user_message(\"Give me a 2 line recipie to make sandwich? By the way I dont like tomatoes in my sandwich\")\n"
      ],
      "metadata": {
        "id": "RBGSMSSGueMR"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see the messages stored in history by using '.messages' method, Observe the \"Human Message\"\n",
        "history.messages\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnviQh1zugqv",
        "outputId": "b9316f0e-6e8e-4a2d-f0df-bd07a6444ae7"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='hi!'),\n",
              " HumanMessage(content='Give me a 2 line recipie to make sandwich? By the way I dont like tomatoes in my sandwich')]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "# Initializing a ChatBot model\n",
        "chat = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "# Getting the AI response by passing the messages in history as input\n",
        "ai_response = chat(history.messages)\n",
        "ai_response\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zzHbljIuiu9",
        "outputId": "1f3bdea1-8716-407e-b709-c31065888ec4"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Spread mayonnaise on two slices of bread, layer with your favorite deli meats and cheese, and enjoy your tomato-free sandwich!')"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now asking a completely random out of context question and storing it in history as well\n",
        "history.add_user_message(\"Which is capital of india?\")\n",
        "ai_response = chat(history.messages)\n",
        "ai_response\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO28wVv1ulZT",
        "outputId": "b6584b02-c6ca-4be6-92ab-4c0f8a57cdcd"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The capital of India is New Delhi.')"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Then when we ask query based on 1st scenario, still by virtue of history\n",
        "# the model is able to remember that tomatoes are disliked\n",
        "history.add_user_message(\"Which ingridient do I not like in my sandwich?\")\n",
        "ai_response = chat(history.messages)\n",
        "ai_response\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA4W6rz1upa4",
        "outputId": "f65318fc-9f6a-491f-f937-2024cd333029"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Sure! Here's a 2-line recipe for a sandwich without tomatoes:\\n\\n1. Spread your favorite condiment on two slices of bread.\\n2. Layer your preferred fillings like lettuce, cheese, and deli meats, then assemble and enjoy!\\n\\n\")"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chains in LLM :--\n",
        "\n",
        "a. Simple Sequential Chain\n",
        "\n",
        "b. Summarization Chain"
      ],
      "metadata": {
        "id": "vsZlzJp8vBH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#a. Simple Sequential Chain"
      ],
      "metadata": {
        "id": "ic9tMHCuv2sX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "llm = OpenAI(temperature=1, openai_api_key=OPENAI_API_KEY)\n"
      ],
      "metadata": {
        "id": "Jo4HxIQdvSWU"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt template that take user input directly\n",
        "template = \"\"\"Your task is to find the movie name which won Academy Awards / Best Picture in the year that user suggests.\n",
        "% YEAR\n",
        "{year}\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"year\"], template=template)\n",
        "# Create chain that will take in the 'year' value and give the name of the movie as output\n",
        "year_chain = LLMChain(llm=llm, prompt=prompt_template)\n"
      ],
      "metadata": {
        "id": "IfmxWDtGvgkW"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt template that will consume output of the previous LLM chain\n",
        "template = \"\"\"Given the movie name, give a short summary of the plot of that movie.\n",
        "% MOVIE_NAME\n",
        "{movie_name}\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"movie_name\"], template=template)\n",
        "# Create chain that will give the plot of the movie give the value of \"movie_name\" variable\n",
        "plot_chain = LLMChain(llm=llm, prompt=prompt_template)\n"
      ],
      "metadata": {
        "id": "uqsLnwoZviuN"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating the Simple Sequential Chain, the order in which chain variable names to \"chains\" argument are passed matters here!!\n",
        "movie_plot_chain = SimpleSequentialChain(chains=[year_chain, plot_chain],\n",
        "\t\t\t\t\t\t\t\t\t\t verbose=True\n",
        "\t\t\t\t\t\t\t\t\t\t )\n"
      ],
      "metadata": {
        "id": "wgO0PXmpvkhb"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Executing the chain, observe that we just gave the year and nothing else,\n",
        "# still the chain found the movie name and also summarized its plot!!\n",
        "review = movie_plot_chain.run(\"2020\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ4ukiRRvm5-",
        "outputId": "d34c90ce-73c9-4236-f982-e4606cdc3a99"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "The movie that won Academy Awards / Best Picture in 2020 is \"Parasite\".\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\"Parasite\" follows the story of a poor family who schemes their way into working for a wealthy family, only to discover dark secrets and hidden agendas that threaten to destroy their newfound success. The film highlights the stark contrast between social classes and the lengths people will go to in order to improve their situation.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#b. Summarization Chain:"
      ],
      "metadata": {
        "id": "Qk_jH7o4v4O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "# Initialize the summary chain\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n"
      ],
      "metadata": {
        "id": "srIdlJcJv1EG"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarizing first 3 documnets from the \"Annual Report\" PDF we saw previously\n",
        "chain.run(pages[1:4])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "oCHhZ6zcwzyB",
        "outputId": "47cae86d-c015-43f9-f49d-d7cc73b7acd6"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nN/A as there is no information provided.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nSySFNzGwzrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Agent (Tools are a comprehensive set of software tools specifically designed to facilitate various language processing and information retrieval tasks. These tools offer a wide range of functionalities, making them invaluable aids for developers and researchers working with language-based projects. Let's delve into the key aspects and utilities of LangChain Tools:\n",
        ")\n",
        "#Tools(Agents offer a unique way of handling applications where the predetermined chain of calls to language models and tools might not suffice. Instead, agents work with unknown chains, which depend on user input, allowing the language model to decide the most appropriate path to take.)"
      ],
      "metadata": {
        "id": "iVR59BdlxiV_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ArPq5hWlxrjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tools concept:\n",
        "\n",
        "By empowering the language model to act as an agent, LangChain opens up new possibilities for applications that require context-aware decision-making and dynamic processing. Agents are an advanced feature that enhances the effectiveness and responsiveness of language-based systems, ensuring they can handle a wide range of user interactions with intelligence and efficiency."
      ],
      "metadata": {
        "id": "_F6SyXfcxva5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class to Load tools based on their name.\n",
        "from langchain.agents import load_tools\n",
        "# class to load an agent executor given the tools and LLM.\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n"
      ],
      "metadata": {
        "id": "fBP03KUMxyKw"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a toolkit (collection of tools), this example only uses \"serpapi\" (search engine) as a tool."
      ],
      "metadata": {
        "id": "CG40vwRLx5Lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwov8_gPyKti",
        "outputId": "d9433221-d5c0-46fa-8d1c-76a013add632"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2023.11.17)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32003 sha256=d25ae2d28d946faae40eb22aecbf7b79263593210d5b1d5983afea52d245bff6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SERPAPI_API_KEY = 'ebfbcbadf20318d6d627f548b57038c586450da60cf3efb232cdb734a36d74d2'\n",
        "toolkit = load_tools(tool_names=[\"serpapi\"],  # there are plenty of tools, which can be used based on requirement\n",
        "                     llm=llm,\n",
        "                    serpapi_api_key=SERPAPI_API_KEY)\n"
      ],
      "metadata": {
        "id": "Q2KJk9Dxx23N"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an agent exutor, this will make decision depending on the scenario and facilitate output generation\n",
        "agent = initialize_agent(toolkit,\n",
        "                         llm,\n",
        "                         agent=\"zero-shot-react-description\",\n",
        "                         verbose=True,\n",
        "                         return_intermediate_steps=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_WuES9HyRDI",
        "outputId": "ec79de7c-2c30-4056-e1f2-bbe159d65fb6"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = '''print what is the name of prime minister of India?\n",
        "                and then in which year he has joined as Prime miniter of India?\n",
        "                Once you find both answers, find how many years has long into this seat.'''\n"
      ],
      "metadata": {
        "id": "GPN_yWt0yVml"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Observe how the agent navigates through LLM outputs using correct tool to arrive at the final answer. The “Action” in this case point to the use of tool.\n",
        "\n",
        "response = agent({\"input\":input_text})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRkCyOeNyYfI",
        "outputId": "8cd0e81d-8956-4357-ec75-687c7d87bad9"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use the search engine to find the answers.\n",
            "Action: Search\n",
            "Action Input: \"prime minister of India\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mNarendra Modi\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Now I need to find the year he joined as Prime Minister.\n",
            "Action: Search\n",
            "Action Input: \"Narendra Modi joined as Prime Minister of India\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m['Modi was the chief minister of Gujarat from 2001 to 2014 and is the Member of Parliament (MP) for Varanasi. He is a member of the Bharatiya Janata Party (BJP) ...', 'Modi was sworn in as prime minister on May 26, 2014. Soon after he took office, his government embarked on several reforms, including campaigns to improve ...', \"Shri Narendra Modi as India's Prime Minister brings with him a rich and hands-on experience as one of India's most successful Chief Ministers and one of its ...\", 'PM Narendra Modi was sworn-in as the Prime Minister of India on 30th May 2019, marking the start of his second term in office. The first ever Prime Minister ...', \"Shri Narendra Modi was sworn-in as India's Prime Minister on 30th May 2019, marking the start of his second term in office.\", 'Prime Minister Narendra Modi, Get all the information news, updates, speeches on the official website on narendramodi.in.', 'Narasimha Rao, Atal Bihari Vajpayee, Manmohan Singh, and Narendra Modi. Modi is the 14th and current prime minister of India, serving since 26 May 2014.', 'On 26th May 2014 Narendra Modi took oath as the Prime Minister of India, becoming the first ever PM to be born after India attained Independence.', \"May 30, 2019 - Sworn in for a second term as India's prime minister. August 5, 2019 - Tensions between India and Pakistan increase after Modi ...\"]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Now I need to find the exact year he joined as Prime Minister.\n",
            "Action: Search\n",
            "Action Input: \"Narendra Modi joined as Prime Minister of India in\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m['Modi was the chief minister of Gujarat from 2001 to 2014 and is the Member of Parliament (MP) for Varanasi. He is a member of the Bharatiya Janata Party (BJP) ...', 'Modi was sworn in as prime minister on May 26, 2014. Soon after he took office, his government embarked on several reforms, including campaigns to improve ...', \"Shri Narendra Modi as India's Prime Minister brings with him a rich and hands-on experience as one of India's most successful Chief Ministers and one of its ...\", 'PM Narendra Modi was sworn-in as the Prime Minister of India on 30th May 2019, marking the start of his second term in office. The first ever Prime Minister ...', \"Shri Narendra Modi was sworn-in as India's Prime Minister on 30th May 2019, marking the start of his second term in office.\", 'Narasimha Rao, Atal Bihari Vajpayee, Manmohan Singh, and Narendra Modi. Modi is the 14th and current prime minister of India, serving since 26 May 2014.', 'On 26th May 2014 Narendra Modi took oath as the Prime Minister of India, becoming the first ever PM to be born after India attained Independence.', 'Prime Minister Narendra Modi, Get all the information news, updates, speeches on the official website on narendramodi.in.', 'Narendra Modi Narendra Modi is an influencer. Government of India Gujarat University. Delhi, India. 4M followers. See your mutual connections ...', 'Here is a look at the life of Narendra Modi, prime minister of India. Personal. Birth date: September 17, 1950.']\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Now I need to calculate how many years he has been in office.\n",
            "Action: Search\n",
            "Action Input: \"Narendra Modi has been in office for\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m[{'title': \"Prime Minister Narendra Modi has 'blessings of divine power', says Congress leader Acharya Pramod - India Today\", 'link': 'https://www.indiatoday.in/india/video/prime-minister-narendra-modi-has-blessings-of-a-divine-power-says-congress-leader-acharya-pramod-2496733-2024-02-02', 'source': 'India Today', 'date': '3 hours ago', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSzP4ykZH69rtInGMILTQdYlk5duMEUXENmBKujgWKR9sn6FXuD7YXL_Vk&usqp=CAI&s'}, {'title': \"Opinion: Is Narendra Modi's India still a democracy?\", 'link': 'https://www.latimes.com/opinion/story/2024-02-02/india-democracy-narendra-modi-hindu-nationalism-ram-temple', 'source': 'Los Angeles Times', 'date': '1 hour ago', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTu8tH_lwEGo8wL8v-HSFTTe_SIBkOMXWZdag4uUzY_UVb1lPeFvF8-pOY&usqp=CAI&s'}, {'title': \"Narendra Modi's government outlines record capex in final pre-election budget\", 'link': 'https://www.ft.com/content/c668f539-1717-4c4b-b4d2-e0096ecef16e', 'source': 'Financial Times', 'date': '1 day ago', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR2sk9nvTtfpknWZBWktI527z_LpjhGqd1ZE4CYDsTEVWVJMI5kBYlKRSMZ&usqp=CAI&s'}]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: Narendra Modi has been in office for 7 years.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Course Over now, Thanks for visiting...<>"
      ],
      "metadata": {
        "id": "-NO5ZWAE0E0a"
      }
    }
  ]
}